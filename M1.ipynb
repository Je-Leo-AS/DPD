{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047bf11c",
   "metadata": {},
   "source": [
    "Como Atividade M1, resolver novamente o exercício da Atividade 2 (conjunto de dados in e out reais contendo 99 amostras), porém, ao invés de identificar os coeficientes usando otimização linear (através da \\\\), usar otimização não linear. Dividir a Atividade M1 em 2 etapas:\n",
    "\n",
    "A. Construir uma function com as seguintes características:\n",
    "\n",
    "1. argumentos (incógnitas) da function: os valores de todos os coeficientes do polinômio de memória (MP);\n",
    "\n",
    "2. processamentos internos dentro da function:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "carregar os sinais de treinamento (in e out);\n",
    "\n",
    "\n",
    "\n",
    "processar o sinal de entrada ao longo do MP (observe que dentro dessa function assume-se que os coeficientes são conhecidos, uma vez que esses valores são passados como argumentos da function);\n",
    "\n",
    "\n",
    "\n",
    "obter a saída estimada pelo MP e calcular o vetor de erro entre saída desejada e saída estimada pela rede;\n",
    "\n",
    "3. retorno da function: vetor de erro.\n",
    "\n",
    "B. Usar otimização não linear para encontrar de uma só vez os valores de todos os coeficientes do polinômio de memória (MP). No Matlab, isso é feito pelo comando lsqnonlin, disponível no Optimization toolbox. No Python eu acredito que também existe um equivalente ao lsqnonlin, mas não sei te passar maiores detalhes. Dicas (que se aplicam ao Matlab):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ver um exemplo disponível no help: help lsqnonlin;\n",
    "\n",
    "\n",
    "\n",
    "no comando lsqnonlin, você chamará a function já pronta na parte A dessa Atividade M1;\n",
    "\n",
    "\n",
    "\n",
    "você deve inserir uma estimativa inicial para o valor de cada coeficiente do MP. Sugiro você testar por exemplo: a) tudo zero; b) tudo um; c) valores aleatórios;\n",
    "\n",
    "\n",
    "\n",
    "ao rodar o lsqnonlin, a cada iteração automaticamente são atualizados os valores dos coeficientes. Você pode visualizar, iteração por iteração, como está o MSE (resnorm) a cada iteração usando 'Display' \"iter\" dentro de optimoptions. (outra dica: fazendo help optimoptions você poderá alterar vários parâmetros do otimizador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a706ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "mat = loadmat('IN_OUT_PA.mat')\n",
    "\n",
    "in_data = mat['in'].reshape(1,-1)[0]\n",
    "out_data = mat['out'].reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09e3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def erro_mp(coef, x_in, y_out, ordem, memoria):\n",
    "    \"\"\"\n",
    "    coef   : vetor de coeficientes do MP\n",
    "    x_in   : sinal de entrada\n",
    "    y_out  : sinal de saída desejada\n",
    "    ordem  : ordem do polinômio (p)\n",
    "    memoria: número de atrasos (M)\n",
    "    \"\"\"\n",
    "    N = len(x_in)\n",
    "    y_est = np.zeros(N)\n",
    "\n",
    "    # Construção do polinômio de memória\n",
    "    for n in range(memoria, N):\n",
    "        soma = 0\n",
    "        idx = 0\n",
    "        for m in range(memoria + 1):\n",
    "            for p in range(1, ordem + 1):\n",
    "                soma += coef[idx] * (x_in[n - m] ** p)\n",
    "                idx += 1\n",
    "        y_est[n] = soma\n",
    "\n",
    "    erro = y_out - y_est\n",
    "    return erro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968ddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         6.2259e+00                                    1.63e+01    \n",
      "       1              2         2.3746e-01      5.99e+00       9.55e-01       4.64e-09    \n",
      "`gtol` termination condition is satisfied.\n",
      "Function evaluations 2, initial cost 6.2259e+00, final cost 2.3746e-01, first-order optimality 4.64e-09.\n",
      "Coeficientes otimizados: [ 0.84151459 -0.11441298 -0.39725129  0.07723682  0.11573277  0.07450171\n",
      " -0.05220898  0.05259112 -0.04325817]\n",
      "MSE final: 0.004797235224108902\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "ordem = 3       \n",
    "memoria = 2     \n",
    "num_coef = ordem * (memoria + 1)\n",
    "\n",
    "x0 = np.zeros(num_coef)   \n",
    "\n",
    "res = least_squares(\n",
    "    erro_mp,\n",
    "    x0,\n",
    "    args=(in_data, out_data, ordem, memoria),\n",
    "    verbose=2  \n",
    ")\n",
    "residuos = res.fun  \n",
    "\n",
    "mse = np.mean(residuos**2)\n",
    "print(\"Coeficientes otimizados:\", res.x)\n",
    "print(\"MSE final:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
